{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886c1063-b6f6-4835-86da-2e2a86d4f1ba",
   "metadata": {},
   "source": [
    "### Method 1. Sentence Transformers to generate embeddings using Multi-GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57661dcd-2d6c-48c8-b29b-e9daf911684b",
   "metadata": {},
   "source": [
    "Importing sentence transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30fbfcd1-760b-4a6a-8a51-184a65b513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39634d3f-e17f-4a4c-b4d4-0790a5688e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d73c0e-619c-449d-8512-e8fd2e03454f",
   "metadata": {},
   "source": [
    "Using one-liner function from sentnece transformers to use multi-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344d60d1-8664-41d4-b16f-f3cf109cb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = model.start_multi_process_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1711b24-60a5-4aa3-a3e6-48ac70c76d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca859fe-8946-4427-99c5-b1537591fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encode_multi_process(sentences, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cd66a8-9142-4320-8dfc-7d97aff2f565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings computed. Shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings computed. Shape:\", emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c278258e-3cc6-4624-8cac-5a1c4fa44f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme/gpu_node/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7637601e10c940ffbd5df81b136f7b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing sentences:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation took 0.42 minutes\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "input_file = 'bnwiki_all_abstract.parquet'\n",
    "output_file = 'hf_embed.parquet'\n",
    "\n",
    "table = pq.read_table(input_file)\n",
    "df_subset = table.slice(0, 500).to_pandas()\n",
    "\n",
    "sentences = df_subset['Abstract'].tolist()\n",
    "version_control = df_subset['Version Control'].tolist()\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\")\n",
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "with tqdm(total=len(sentences), desc=\"Processing sentences\") as pbar:\n",
    "    embeddings = model.encode_multi_process(sentences, pool)\n",
    "    pbar.update(len(sentences))\n",
    "\n",
    "# Convert embeddings to lists of floats\n",
    "embedding_lists = [list(embed) for embed in embeddings]\n",
    "\n",
    "# Define schema for the table\n",
    "embedding_field = pa.field('embeddings', pa.list_(pa.float32()))\n",
    "version_control_field = pa.field('Version Control', pa.string())\n",
    "schema = pa.schema([embedding_field, version_control_field])\n",
    "\n",
    "# Create a PyArrow Table\n",
    "embedding_array = pa.array(embedding_lists, type=pa.list_(pa.float32()))\n",
    "embedding_dict = {\n",
    "    'embeddings': embedding_array,\n",
    "    'Version Control': pa.array(version_control)\n",
    "}\n",
    "table = pa.table(embedding_dict, schema=schema)\n",
    "\n",
    "pq.write_table(table, output_file)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"Operation took {elapsed_time:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22549a7-a8de-4d50-a869-59cada358729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Embeddings   Version Control\n",
      "0    [-0.53130025, 0.57009095, -0.30176148, -0.4871...      MjAyNDA4MTkx\n",
      "1    [-0.916644, 1.1611831, -0.46351388, 0.11054788...      MjAyNDA4MTky\n",
      "2    [0.27030998, 0.3748276, -0.8021263, -0.0353029...      MjAyNDA4MTkz\n",
      "3    [-0.0006129769, 0.10446445, -0.91086644, -0.41...      MjAyNDA4MTk0\n",
      "4    [-0.92064404, 0.9358199, -0.8850981, -0.496975...      MjAyNDA4MTk1\n",
      "..                                                 ...               ...\n",
      "995  [-0.39280245, -0.21199724, -0.2767391, -0.6167...  MjAyNDA4MTk5OTY=\n",
      "996  [0.02107662, 0.6222971, -0.31643283, -0.400008...  MjAyNDA4MTk5OTc=\n",
      "997  [-1.2849243, 1.0070957, -0.7613241, -0.1102534...  MjAyNDA4MTk5OTg=\n",
      "998  [-1.2849243, 1.0070957, -0.7613241, -0.1102534...  MjAyNDA4MTk5OTk=\n",
      "999  [-1.2849243, 1.0070957, -0.7613241, -0.1102534...  MjAyNDA4MTkxMDAw\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "input_file = 'hf_embed.parquet'\n",
    "\n",
    "table = pq.read_table(input_file)\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476770d3-79fc-47b6-b03d-584c3542857a",
   "metadata": {},
   "source": [
    "### Method 2. Generate Embeddings using Ray and Distributing the workload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf2256b-c795-4536-ab4b-1e48003d6466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initializing model...\n",
      "Processing batches...\n",
      "Combining embeddings...\n",
      "Saving results...\n",
      "Processing completed in 0.42 minutes.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_parquet('bnwiki_all_abstract.parquet')\n",
    "df = df.head(300)\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model_name = 'BAAI/bge-m3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(accelerator.device)\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "def compute_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = {key: value.to(accelerator.device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "version_control = df['Version Control'].tolist()\n",
    "\n",
    "print(\"Processing batches...\")\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['Abstract'].iloc[i:i + batch_size].tolist()\n",
    "    batch_embeddings = compute_embeddings(batch_texts, tokenizer, model)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "print(\"Combining embeddings...\")\n",
    "all_embeddings = torch.cat(embeddings).cpu().numpy()\n",
    "\n",
    "print(\"Saving results...\")\n",
    "output_df = pd.DataFrame({\n",
    "    'Embeddings': list(all_embeddings),\n",
    "    'Version Control': version_control\n",
    "})\n",
    "output_df.to_parquet('hf_embed.parquet', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"Processing completed in {elapsed_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85a1056-897a-4aa8-8202-fae4ed17696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  2 17:07:09 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 34%   25C    P8             36W /  370W |    7240MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off |   00000000:4F:00.0 Off |                  N/A |\n",
      "| 41%   25C    P8             35W /  370W |      14MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2390      G   /usr/lib/xorg/Xorg                              8MiB |\n",
      "|    0   N/A  N/A      2469      G   /usr/bin/gnome-shell                            6MiB |\n",
      "|    0   N/A  N/A   2416197      C   python3                                      1362MiB |\n",
      "|    0   N/A  N/A   2632641      C   python                                       2016MiB |\n",
      "|    0   N/A  N/A   2936172      C   python                                       3828MiB |\n",
      "|    1   N/A  N/A      2390      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e031ecac-f815-4d92-b726-b7393338922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initializing model...\n",
      "Processing batches...\n",
      "Combining embeddings...\n",
      "Saving results...\n",
      "Processing completed in 0.60 minutes.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_parquet('bnwiki_all_abstract.parquet')\n",
    "df = df.head(500)\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model_name = 'BAAI/bge-m3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "def compute_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = {key: value.to(accelerator.device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "version_control = df['Version Control'].tolist()\n",
    "\n",
    "print(\"Processing batches...\")\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['Abstract'].iloc[i:i + batch_size].tolist()\n",
    "    with accelerator.split_between_processes(batch_texts) as split_batch_texts:\n",
    "        batch_embeddings = compute_embeddings(split_batch_texts, tokenizer, model)\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "print(\"Combining embeddings...\")\n",
    "all_embeddings = torch.cat(embeddings).cpu().numpy()\n",
    "\n",
    "print(\"Saving results...\")\n",
    "output_df = pd.DataFrame({\n",
    "    'Embeddings': list(all_embeddings),\n",
    "    'Version Control': version_control\n",
    "})\n",
    "output_df.to_parquet('hf_embed.parquet', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"Processing completed in {elapsed_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e5f4ea-77fc-4338-bddf-0660bc4aa949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initializing model...\n",
      "Processing batches...\n",
      "Combining embeddings...\n",
      "Saving results...\n",
      "Processing completed in 1.05 minutes.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator, PartialState\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "accelerator = Accelerator()\n",
    "distributed_state = PartialState()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_parquet('bnwiki_all_abstract.parquet')\n",
    "df = df.head(1000)\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model_name = 'BAAI/bge-m3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(accelerator.device)\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "def compute_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = {key: value.to(accelerator.device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "version_control = df['Version Control'].tolist()\n",
    "\n",
    "print(\"Processing batches...\")\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['Abstract'].iloc[i:i + batch_size].tolist()\n",
    "    with distributed_state.split_between_processes(batch_texts) as split_batch_texts:\n",
    "        batch_embeddings = compute_embeddings(split_batch_texts, tokenizer, model)\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "print(\"Combining embeddings...\")\n",
    "all_embeddings = torch.cat(embeddings).cpu().numpy()\n",
    "\n",
    "print(\"Saving results...\")\n",
    "output_df = pd.DataFrame({\n",
    "    'Embeddings': list(all_embeddings),\n",
    "    'Version Control': version_control\n",
    "})\n",
    "output_df.to_parquet('hf_embed.parquet', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"Processing completed in {elapsed_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6450661-7eaa-49f9-abc1-b010111ab863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c180bef-c002-4b59-bff6-0aaeeb4c4615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
